{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tetris_NN_value_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c3a576543842>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m     \u001b[0mmodel_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tetris_NN_value_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m     \u001b[0mtetris_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tetris_NN_value_model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "# GPU if available\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BOARD_WIDTH = 10\n",
    "BOARD_HEIGHT = 20\n",
    "BLANK = 0\n",
    "\n",
    "TEMPLATE_WIDTH = 5\n",
    "TEMPLATE_HEIGHT = 5\n",
    "\n",
    "S_SHAPE_TEMPLATE = [['.....',\n",
    "                     '.....',\n",
    "                     '..OO.',\n",
    "                     '.OO..',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '..O..',\n",
    "                     '..OO.',\n",
    "                     '...O.',\n",
    "                     '.....']]\n",
    "\n",
    "Z_SHAPE_TEMPLATE = [['.....',\n",
    "                     '.....',\n",
    "                     '.OO..',\n",
    "                     '..OO.',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '..O..',\n",
    "                     '.OO..',\n",
    "                     '.O...',\n",
    "                     '.....']]\n",
    "\n",
    "I_SHAPE_TEMPLATE = [['..O..',\n",
    "                     '..O..',\n",
    "                     '..O..',\n",
    "                     '..O..',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '.....',\n",
    "                     'OOOO.',\n",
    "                     '.....',\n",
    "                     '.....']]\n",
    "\n",
    "O_SHAPE_TEMPLATE = [['.....',\n",
    "                     '.....',\n",
    "                     '.OO..',\n",
    "                     '.OO..',\n",
    "                     '.....']]\n",
    "\n",
    "J_SHAPE_TEMPLATE = [['.....',\n",
    "                     '.O...',\n",
    "                     '.OOO.',\n",
    "                     '.....',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '..OO.',\n",
    "                     '..O..',\n",
    "                     '..O..',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '.....',\n",
    "                     '.OOO.',\n",
    "                     '...O.',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '..O..',\n",
    "                     '..O..',\n",
    "                     '.OO..',\n",
    "                     '.....']]\n",
    "\n",
    "L_SHAPE_TEMPLATE = [['.....',\n",
    "                     '...O.',\n",
    "                     '.OOO.',\n",
    "                     '.....',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '..O..',\n",
    "                     '..O..',\n",
    "                     '..OO.',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '.....',\n",
    "                     '.OOO.',\n",
    "                     '.O...',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '.OO..',\n",
    "                     '..O..',\n",
    "                     '..O..',\n",
    "                     '.....']]\n",
    "\n",
    "T_SHAPE_TEMPLATE = [['.....',\n",
    "                     '..O..',\n",
    "                     '.OOO.',\n",
    "                     '.....',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '..O..',\n",
    "                     '..OO.',\n",
    "                     '..O..',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '.....',\n",
    "                     '.OOO.',\n",
    "                     '..O..',\n",
    "                     '.....'],\n",
    "                    ['.....',\n",
    "                     '..O..',\n",
    "                     '.OO..',\n",
    "                     '..O..',\n",
    "                     '.....']]\n",
    "\n",
    "PIECES = {'S': S_SHAPE_TEMPLATE,\n",
    "          'Z': Z_SHAPE_TEMPLATE,\n",
    "          'J': J_SHAPE_TEMPLATE,\n",
    "          'L': L_SHAPE_TEMPLATE,\n",
    "          'I': I_SHAPE_TEMPLATE,\n",
    "          'O': O_SHAPE_TEMPLATE,\n",
    "          'T': T_SHAPE_TEMPLATE}\n",
    "\n",
    "PIECES_IND = {'S': 0,\n",
    "              'Z': 1,\n",
    "              'J': 2,\n",
    "              'L': 3,\n",
    "              'I': 4,\n",
    "              'O': 5,\n",
    "              'T': 6}\n",
    "\n",
    "PIECES_MARGINS = {'S': [[1, 1, 0], [0, 1, 1]],\n",
    "                  'Z': [[1, 1, 0], [1, 0, 1]],\n",
    "                  'J': [[1, 1, 1], [0, 1, 1], [1, 1, 0], [1, 0, 1]],\n",
    "                  'L': [[1, 1, 1], [0, 1, 1], [1, 1, 0], [1, 0, 1]],\n",
    "                  'I': [[0, 0, 2], [2, 1, 0]],\n",
    "                  'O': [[1, 0, 0]],\n",
    "                  'T': [[1, 1, 1], [0, 1, 1], [1, 1, 0], [1, 0, 1]]}\n",
    "\n",
    "\n",
    "class Tetris:\n",
    "    def __init__(self):\n",
    "        self.board = self.getBlankBoard()\n",
    "        self.current_piece = self.getNewPiece()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Restarts the game with a blank board and new piece.\n",
    "        @rtype: torch tensor\n",
    "            A tensor representing the state.\n",
    "        \"\"\"\n",
    "        self.board = self.getBlankBoard()\n",
    "        self.current_piece = self.getNewPiece()\n",
    "        return self.convertToFeatures(self.board)\n",
    "\n",
    "    def isOnBoard(self, x, y):\n",
    "        \"\"\"\n",
    "        Checks if the position (x,y) is on the board.\n",
    "        @type x: int\n",
    "            The x position\n",
    "        @type y: int\n",
    "            The y position\n",
    "\n",
    "        @rtype: Boolean\n",
    "            If (x,y) is on the board.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0 <= x < BOARD_WIDTH and 0 <= y < BOARD_HEIGHT\n",
    "\n",
    "    def getBlankBoard(self):\n",
    "        \"\"\"\n",
    "        Returns a blank board.\n",
    "        \"\"\"\n",
    "        return np.zeros((BOARD_WIDTH, BOARD_HEIGHT))\n",
    "\n",
    "    def isValidPosition(self, board, piece, x, y, rotation):\n",
    "        \"\"\"\n",
    "        Checks if a piece has a valid position on the board.\n",
    "        @type board: np.array\n",
    "            A np array representing the state of the board.\n",
    "        @type piece: string\n",
    "            A string representing the shape of the piece.\n",
    "        @type x: int\n",
    "            The x position of the piece.\n",
    "        @type y: int\n",
    "            The y position of the piece.\n",
    "        @rotation: int\n",
    "            The rotation of the piece.\n",
    "        @rtype: Boolean\n",
    "            If the piece has a valid position on the board.\n",
    "\n",
    "        \"\"\"\n",
    "        for dx in range(TEMPLATE_WIDTH):\n",
    "            for dy in range(TEMPLATE_HEIGHT):\n",
    "                template = PIECES[piece][rotation % len(PIECES[piece])]\n",
    "                if template[dy][dx] == 'O':\n",
    "                    board_x_pos, board_y_pos = x + (dx - 2), y - (dy - 2)\n",
    "                    if not self.isOnBoard(board_x_pos, board_y_pos) or board[board_x_pos][board_y_pos]:\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    def getNewPiece(self):\n",
    "        \"\"\"\n",
    "        Gets a new piece.\n",
    "        @rtype: string\n",
    "            A string representing the shape of the new piece.\n",
    "        \"\"\"\n",
    "        return random.choice(list(PIECES.keys()))\n",
    "\n",
    "    def findXYCoordinate(self, piece, action, board):\n",
    "        \"\"\"\n",
    "        Find the x and y coordinates to place a piece given an action.\n",
    "        @type piece: string\n",
    "            A letter representing the shape of the piece to be placed.\n",
    "        @type action: int\n",
    "            An integer representing the action.\n",
    "        @type board: np.array\n",
    "            A np array representing the state of the board.\n",
    "        @rtype: tuple[int]\n",
    "            A tuple (x, y, rotation) representing the (x,y) coordinates of the\n",
    "            piece if it were to be placed on the board as well as the rotation\n",
    "            of the piece. Note that this does not actually place the piece.\n",
    "        \"\"\"\n",
    "        rotation = action % 4\n",
    "        left_margin, right_margin, top_margin = PIECES_MARGINS[piece][\n",
    "            rotation % len(PIECES_MARGINS[piece])]\n",
    "        x = max(left_margin, min(action // 4, BOARD_WIDTH - right_margin - 1))\n",
    "\n",
    "        # Finding y coordinate to place the piece\n",
    "        valid_y = None\n",
    "        flag = False\n",
    "        for y in range(BOARD_HEIGHT - top_margin - 1, -2, -1):\n",
    "            if self.isValidPosition(board, piece, x, y, rotation):\n",
    "                flag = True\n",
    "            else:\n",
    "                if flag:\n",
    "                    valid_y = y + 1\n",
    "                break\n",
    "\n",
    "        return x, valid_y, rotation\n",
    "\n",
    "    def transitionState(self, action):\n",
    "        \"\"\"\n",
    "        Returns the next state given the action.\n",
    "        @type action: int\n",
    "            An integer representing the action chosen.\n",
    "            In total, there are BOARD_WIDTH x 4 actions, representing\n",
    "            choices in the x coordinate and rotation of the piece.\n",
    "            For a chosen x and rotation r, the action is 4 * x + r.\n",
    "\n",
    "        @rtype: tuple\n",
    "            A tuple (reward, next_state, done) representing the reward, next state,\n",
    "            and if the game has finished.\n",
    "\n",
    "        \"\"\"\n",
    "        x, y, rotation = self.findXYCoordinate(self.current_piece, action, self.board)\n",
    "\n",
    "        if y != None:\n",
    "            self.board = self.placeOnBoard(self.current_piece, x, y, rotation, self.board)\n",
    "\n",
    "            self.current_piece = self.getNewPiece()\n",
    "            next_state = self.convertToFeatures(self.board)\n",
    "\n",
    "            self.board, lines_cleared = self.clearLines(self.board)\n",
    "            #delta_r, delta_c = self.countHoles(self.board)\n",
    "            #reward = lines_cleared**2 - delta_r - delta_c + 0.1\n",
    "            reward = 0.1 + (lines_cleared+1)**2\n",
    "\n",
    "            return reward, next_state, False, lines_cleared\n",
    "\n",
    "        return -1, self.convertToFeatures(self.board), True, 0\n",
    "\n",
    "    def getAllNextStates(self):\n",
    "        \"\"\"\n",
    "        Get all of the next states corresponding to all possible next actions.\n",
    "        @rtype: list[tuple]\n",
    "            A list of tuples (action, features) representing the features of the\n",
    "            next state if an action is taken.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for action in range(BOARD_WIDTH * 4):\n",
    "            x, y, rotation = self.findXYCoordinate(self.current_piece, action, self.board)\n",
    "            if y != None:\n",
    "                self.board = self.placeOnBoard(self.current_piece, x, y, rotation, self.board)\n",
    "                features = self.convertToFeatures(self.board)\n",
    "                data.append((action, features))\n",
    "                self.board = self.removeFromBoard(self.current_piece, x, y, rotation, self.board)\n",
    "        return data\n",
    "\n",
    "    def removeFromBoard(self, piece, x, y, rotation, board):\n",
    "        \"\"\"\n",
    "        removes the current piece on the board.\n",
    "        @type piece: string\n",
    "            A letter representing the shape of the piece.\n",
    "        @type x: int\n",
    "            The x position of the piece.\n",
    "        @type y: int\n",
    "            The y position of the piece.\n",
    "        @type rotation: int\n",
    "            The rotation of the piece.\n",
    "        @type board: np.array\n",
    "            A np array representing the board.\n",
    "        @type: np.array\n",
    "            A np array representing the board after the piece has been removed.\n",
    "        \"\"\"\n",
    "        template = PIECES[piece][rotation % len(PIECES[piece])]\n",
    "        for dx in range(TEMPLATE_WIDTH):\n",
    "            for dy in range(TEMPLATE_HEIGHT):\n",
    "                if template[dy][dx] == 'O':\n",
    "                    board_x_pos, board_y_pos = x + (dx - 2), y - (dy - 2)\n",
    "                    board[board_x_pos][board_y_pos] = 0.0\n",
    "\n",
    "        return board\n",
    "\n",
    "    def placeOnBoard(self, piece, x, y, rotation, board):\n",
    "        \"\"\"\n",
    "        Places the current piece on the board. Assumes that the piece\n",
    "        is in a valid position.\n",
    "        @type piece: string\n",
    "            A letter representing the shape of the piece.\n",
    "        @type x: int\n",
    "            The x position of the piece.\n",
    "        @type y: int\n",
    "            The y position of the piece.\n",
    "        @type rotation: int\n",
    "            The rotation of the piece.\n",
    "        @type board: np.array\n",
    "            A np array representing the board.\n",
    "        @type: np.array\n",
    "            A np array representing the board after the piece has been placed.\n",
    "        \"\"\"\n",
    "        template = PIECES[piece][rotation % len(PIECES[piece])]\n",
    "        for dx in range(TEMPLATE_WIDTH):\n",
    "            for dy in range(TEMPLATE_HEIGHT):\n",
    "                if template[dy][dx] == 'O':\n",
    "                    board_x_pos, board_y_pos = x + (dx - 2), y - (dy - 2)\n",
    "                    board[board_x_pos][board_y_pos] = 1.0\n",
    "\n",
    "        return board\n",
    "\n",
    "    def clearLines(self, board):\n",
    "        \"\"\"\n",
    "        Removes completed lines from the board.\n",
    "        @rtype: int\n",
    "            The number of lines removed.\n",
    "\n",
    "        \"\"\"\n",
    "        lines_removed = 0\n",
    "        y = 0  # start y at the bottom of the board\n",
    "        while y < BOARD_HEIGHT:\n",
    "            if self.isCompleteLine(y, board):\n",
    "                # Remove the line and pull boxes down by one line.\n",
    "                for pull_down_Y in range(y, BOARD_HEIGHT - 1):\n",
    "                    for x in range(BOARD_WIDTH):\n",
    "                        board[x][pull_down_Y] = board[x][pull_down_Y + 1]\n",
    "                # Set very top line to blank.\n",
    "                for x in range(BOARD_WIDTH):\n",
    "                    board[x][BOARD_HEIGHT - 1] = BLANK\n",
    "                lines_removed += 1\n",
    "                # Note on the next iteration of the loop, y is the same.\n",
    "                # This is so that if the line that was pulled down is also\n",
    "                # complete, it will be removed.\n",
    "            else:\n",
    "                y += 1  # move on to check next row up\n",
    "        return board, lines_removed\n",
    "\n",
    "    def countCompleteLines(self, board):\n",
    "        \"\"\"\n",
    "        Counts the number of completed lines.\n",
    "        @type board: np.array\n",
    "            An np array representing the board.\n",
    "        @rtype: int\n",
    "            The number of completed lines on the board.\n",
    "        \"\"\"\n",
    "        completed_lines = 0\n",
    "        for y in range(BOARD_HEIGHT):\n",
    "            if self.isCompleteLine(y, board):\n",
    "                completed_lines += 1\n",
    "\n",
    "        return completed_lines\n",
    "\n",
    "    def isCompleteLine(self, y, board):\n",
    "        \"\"\"\n",
    "        Checks if the line at height y is complete.\n",
    "        @type y: int\n",
    "            The height of the row to check.\n",
    "\n",
    "        @rtype : Boolean\n",
    "            True if the row is complete.\n",
    "\n",
    "        \"\"\"\n",
    "        for x in range(BOARD_WIDTH):\n",
    "            if board[x][y] == 0.0: return False\n",
    "        return True\n",
    "\n",
    "    def convertToFeatures(self, board):\n",
    "        \"\"\"\n",
    "        Converts the current board position and falling piece to a\n",
    "        list of features.\n",
    "        The features consist of:\n",
    "            - Number of holes along the vertical and horizontal directions.\n",
    "            - Total height of all columns.\n",
    "            - Bumpiness.\n",
    "            - Number of completed lines on the board.\n",
    "        @rtype: torch tensor\n",
    "            Torch tensor of the features described above. Values normalized to be between -1 and 1.\n",
    "\n",
    "        \"\"\"\n",
    "        delta_r, delta_c = self.countHoles(board)\n",
    "        total_height, bumpiness = self.scoreBumpiness(board)\n",
    "        completed_lines = self.countCompleteLines(board)\n",
    "\n",
    "        return torch.tensor([[delta_r/BOARD_HEIGHT, delta_c/BOARD_WIDTH,\n",
    "                              total_height/BOARD_WIDTH,\n",
    "                              bumpiness/BOARD_WIDTH, completed_lines*3]],\n",
    "                            dtype=torch.float32).to(device)\n",
    "\n",
    "    def countHoles(self, board):\n",
    "        \"\"\"\n",
    "        Counts the number of transitions from filled to empty or vice\n",
    "        versa in the rows and columns.\n",
    "        @rtype: tuple[int]\n",
    "            A tuple (delta_r, delta_c) representing the number of transitions\n",
    "            from filled to empty squares or vice versa across rows and columns respectively.\n",
    "\n",
    "        \"\"\"\n",
    "        # Across rows:\n",
    "        delta_r = 0\n",
    "        for y in range(BOARD_HEIGHT):\n",
    "            for x in range(BOARD_WIDTH - 1):\n",
    "                if board[x][y] != board[x + 1][y]:\n",
    "                    delta_r += 1\n",
    "\n",
    "        # Across columns:\n",
    "        delta_c = 0\n",
    "        for x in range(BOARD_WIDTH):\n",
    "            for y in range(BOARD_HEIGHT - 1):\n",
    "                if board[x][y] != board[x][y + 1]:\n",
    "                    delta_c += 1\n",
    "\n",
    "        return delta_r, delta_c\n",
    "\n",
    "    def scoreBumpiness(self, board):\n",
    "        \"\"\"\n",
    "        Calculates the \"bumpiness\" of the board, defined as the\n",
    "        sum of the absolute differences in heights of adjacent columns,\n",
    "        except for the largest difference.\n",
    "\n",
    "        @type board: np.array\n",
    "            A np array representing the board.\n",
    "        @rtype: tuple[int]\n",
    "            A tuple (max_height, bumpiness) representing the\n",
    "            maximum height of any column and the bumpiness.\n",
    "        \"\"\"\n",
    "        heights = []\n",
    "\n",
    "        for x in range(BOARD_WIDTH):\n",
    "            for y in range(BOARD_HEIGHT-1, -1, -1):\n",
    "                if board[x][y]: break\n",
    "            heights.append(y)\n",
    "\n",
    "\n",
    "        diffs = [abs(heights[i]-heights[i-1]) for i in range(1, len(heights))]\n",
    "        diffs.sort()\n",
    "\n",
    "        return sum(heights), sum(diffs)-diffs[-1]\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initializes the replay memory.\n",
    "        @type capacity: int\n",
    "            The capacity of the memory.\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\n",
    "        \"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a batch of transitions of size batch_size\n",
    "        randomly.\n",
    "        @type batch_size: int\n",
    "            The size of the batch.\n",
    "        @rtype: list[tuple]\n",
    "            A list of the sampled transitions.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\" Agent object that uses the actor-critic network to find the\n",
    "        optimal policy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, NN, optimizer, criterion):\n",
    "        \"\"\" Initializes the agent.\n",
    "\n",
    "            @type env: Tetris\n",
    "                The Tetris environment.\n",
    "            @type NN: NeuralNet\n",
    "                Neural network for computing the state-action values.\n",
    "            @type optimizer: torch.optim\n",
    "                Torch optimizer object.\n",
    "            @type criterion: nn loss\n",
    "                Neural network loss function.\n",
    "            \"\"\"\n",
    "        self.env = env\n",
    "        self.NN = NN\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def chooseAction(self, epsilon):\n",
    "        \"\"\" Chooses action. With probability epsilon it will choose\n",
    "            an exploratory action. Otherwise, it will choose\n",
    "            an action which maximizes the estimated reward of all\n",
    "            possible next states.\n",
    "\n",
    "            @type epsilon: float\n",
    "                Exploration probability.\n",
    "            @rtype: int\n",
    "                An integer representing the action.\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return random.randrange(BOARD_WIDTH * 4)\n",
    "\n",
    "        cur_best_val = -float('inf')\n",
    "        cur_best_action = 0\n",
    "\n",
    "        data = env.getAllNextStates()\n",
    "\n",
    "        for action, state in data:\n",
    "            value = self.NN(state).item()\n",
    "            if value > cur_best_val:\n",
    "                cur_best_val = value\n",
    "                cur_best_action = action\n",
    "\n",
    "        return cur_best_action\n",
    "\n",
    "    def optimizeModel(self, memory, batch_size, gamma):\n",
    "        \"\"\"\n",
    "        Performs one step of mini-batch gradient descent.\n",
    "\n",
    "        @type memory: ReplayMemory\n",
    "            The replay memory from which to draw the experience.\n",
    "        @type batch_size: int\n",
    "            The mini-batch size.\n",
    "        @type gamma: float\n",
    "            The discount factor.\n",
    "        \"\"\"\n",
    "        batch_size = min(len(memory), batch_size)\n",
    "\n",
    "        # Sampling experiences\n",
    "        transitions = memory.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Unpacking data\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_batch = torch.cat(batch.done)\n",
    "\n",
    "        # Predictions and targets\n",
    "        predictions = self.NN(state_batch)\n",
    "        with torch.no_grad():\n",
    "            targets = reward_batch + gamma * self.NN(next_state_batch) * done_batch\n",
    "\n",
    "        # Loss and gradient descent\n",
    "        loss = self.criterion(predictions, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def train(self, episodes, epsilon_initial, epsilon_min, epsilon_stop_episode, gamma, memory_capacity, batch_size):\n",
    "        \"\"\" Trains the agent using the actor-critic method with eligibility traces.\n",
    "\n",
    "            @type episodes: int\n",
    "                The number of episodes to train.\n",
    "            @type epsilon: float\n",
    "                The exploration probability.\n",
    "            @type gamma: float\n",
    "                The discount factor.\n",
    "            @type memory_capacity: int\n",
    "                The capacity of the replay memory.\n",
    "            @type batch_size: int\n",
    "                Mini-batch size for training.\n",
    "\n",
    "        \"\"\"\n",
    "        %matplotlib\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        fig.show()\n",
    "        fig.canvas.draw()\n",
    "        plt.grid()\n",
    "        plt.xlim(-0.5, BOARD_WIDTH - 0.5)\n",
    "        plt.ylim(BOARD_HEIGHT - 0.5, -0.5)\n",
    "\n",
    "        memory = ReplayMemory(memory_capacity)\n",
    "\n",
    "        tot_steps = 0\n",
    "\n",
    "        LC = 0\n",
    "\n",
    "        depsilon = (epsilon_initial-epsilon_min)/epsilon_stop_episode\n",
    "\n",
    "        for episode in range(episodes):\n",
    "\n",
    "            if epsilon_initial > epsilon_min:\n",
    "                epsilon_initial -= depsilon\n",
    "\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f'Episode {episode + 1}/{episodes} completed!')\n",
    "                torch.save(self.NN.state_dict(), 'tetris_NN_value_model')\n",
    "                print(f'Average steps per episode: {tot_steps / 10}')\n",
    "                print(f'Average lines cleared per episode: {LC / 10}')\n",
    "\n",
    "                tot_steps = 0\n",
    "                LC = 0\n",
    "\n",
    "            state, done = self.env.reset(), False\n",
    "\n",
    "            # Initialize eligibility traces\n",
    "\n",
    "            while not done:\n",
    "                tot_steps += 1\n",
    "\n",
    "                time.sleep(0.2)\n",
    "                plt.imshow(np.transpose(self.env.board)[::-1], cmap=plt.cm.binary, interpolation='none', origin='lower')\n",
    "                ax = plt.gca()\n",
    "                ax.set_xticks(np.arange(-0.5, BOARD_WIDTH - 0.5, 1))\n",
    "                ax.set_yticks(np.arange(0.5, BOARD_HEIGHT - 0.5, 1))\n",
    "                fig.canvas.draw()\n",
    "\n",
    "                action = self.chooseAction(epsilon_initial)\n",
    "\n",
    "                reward, next_state, done, lines_cleared = self.env.transitionState(action)\n",
    "                reward = torch.tensor([[reward]], device=device)\n",
    "                done = torch.tensor([[done]], device=device)\n",
    "\n",
    "                # Saves the transition\n",
    "                memory.push(state, next_state, reward, done)\n",
    "\n",
    "                LC += lines_cleared\n",
    "\n",
    "                # Perform one step of batch gradient descent\n",
    "                self.optimizeModel(memory, batch_size, gamma)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.l2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.l3 = nn.Linear(hidden_size2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Network parameters\n",
    "    input_size = 5\n",
    "    hidden_size1 = 32\n",
    "    hidden_size2 = 16\n",
    "\n",
    "    # Training parameters\n",
    "    episodes = 1000000\n",
    "    gamma = 0.8\n",
    "    learning_rate = 2e-3\n",
    "    epsilon_initial = 0.1\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_stop_episode = 1500\n",
    "    memory_capacity = 10000\n",
    "    batch_size = 16\n",
    "\n",
    "    env = Tetris()\n",
    "    model_value = QNetwork(input_size, hidden_size1, hidden_size2).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model_value.parameters(), lr=learning_rate)\n",
    "\n",
    "    model_value.load_state_dict(torch.load('tetris_NN_value_model'))\n",
    "\n",
    "    tetris_agent = Agent(env, model_value, optimizer, criterion)\n",
    "\n",
    "    tetris_agent.train(episodes, epsilon_initial, epsilon_min, epsilon_stop_episode, gamma, memory_capacity, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
